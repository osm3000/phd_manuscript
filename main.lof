\select@language {english}
\contentsline {xpart}{Introduction and Problem Description}{19}{part.1}
\addvspace {10\p@ }
\contentsline {xchapter}{Introduction}{19}{chapter.1}
\addvspace {10\p@ }
\contentsline {xchapter}{Datasets}{23}{chapter.2}
\contentsline {figure}{\numberline {2.1}{\ignorespaces An example from \textit {IRONOFF} (letter 'a'). To the left, we have the image of the letter (i.e., offline-handwriting). To the right is an example of the format for the online-handwriting. We have the writer's ID, origin, handiness, age and gender. The sequence of pen movement to draw the letter are then given: pen state (PEN\_DOWN, PEN\_UP), X, Y coordinates, pen pressure, and time.\relax }}{26}{figure.caption.4}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Summary statistics about the writers in \textit {IRONOFF}: the age, gender, country and handiness.\relax }}{27}{figure.caption.5}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Summary statistics strokes for all categories in \textit {IRONOFF} dataset (uppercase and lowercase letters, and digits), starting from the simplest to the more complex.\relax }}{28}{figure.caption.6}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Drawing time for all categories in \textit {IRONOFF} dataset, arranged from the smallest to the largest.\relax }}{29}{figure.caption.7}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Pausing time for all categories in \textit {IRONOFF} dataset, arranged from the smallest to the largest.\relax }}{30}{figure.caption.8}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Examples from different categories in \textit {QuickDraw!} dataset. Source of the images are \citep {quickdraw}\relax }}{32}{figure.caption.9}
\contentsline {figure}{\numberline {2.7}{\ignorespaces The distribution of the strokes in \textit {QuickDraw!} for the recognized shapes, for each category.\relax }}{35}{figure.caption.10}
\contentsline {figure}{\numberline {2.8}{\ignorespaces The recognized VS non-recognized drawings in \textit {QuickDraw!} in each of the selected categories.\relax }}{36}{figure.caption.11}
\contentsline {figure}{\numberline {2.9}{\ignorespaces QuickDraw! strokes statistics for each of the selected categories.\relax }}{37}{figure.caption.12}
\contentsline {figure}{\numberline {2.10}{\ignorespaces QuickDraw! pausing time statistics for each of the selected categories.\relax }}{38}{figure.caption.13}
\contentsline {figure}{\numberline {2.11}{\ignorespaces QuickDraw! drawing time statistics for each of the selected categories.\relax }}{39}{figure.caption.14}
\contentsline {figure}{\numberline {2.12}{\ignorespaces The most dominant countries for the players in QuickDraw! dataset. In the sample we analyzed, there is players from around 160 countries, but the majorty are from United States, followed by Great Britain.\relax }}{40}{figure.caption.15}
\contentsline {figure}{\numberline {2.13}{\ignorespaces Example of a single-hidden layer neural network. The circle units is the sum of the weighted neurons connected to this unit, and the square units is the application of the activation function on that sum.\relax }}{42}{figure.caption.17}
\contentsline {figure}{\numberline {2.14}{\ignorespaces The performance of the simple linear activation function on two setups. Left: in case of one-to-one mapping between the input and the output, it performs well. Right: In case of one-to-many mapping, the function starts to average over the seen observations, leading to undesirable behaviour. Source of this image is \citep {ha2015mdntf}.\relax }}{43}{figure.caption.18}
\contentsline {figure}{\numberline {2.15}{\ignorespaces Example for freeman code representation for 8 directions. Each direction is given a unique number.\relax }}{46}{figure.caption.20}
\contentsline {xpart}{Experiments}{51}{part.2}
\addvspace {10\p@ }
\contentsline {xchapter}{Generation, benchmarks and evaluation}{51}{chapter.3}
\contentsline {figure}{\numberline {3.1}{\ignorespaces A demonstration of how RNN works: the network is applied on each token in the input ($x_1, x_2, ..., x_t$), while update the hidden state variable every time ($h_0, h_1, ..., h_t$). The output at each step is a function of the hidden state variable (not demonstrated here). Source of the image is (\citep {howrnnworks}).\relax }}{56}{figure.caption.22}
\contentsline {figure}{\numberline {3.2}{\ignorespaces A demonstration for how a gradient descent algorithm work. The optimization process progress each step towards the global optima (the indicated point in the center).\relax }}{58}{figure.caption.23}
\contentsline {figure}{\numberline {3.3}{\ignorespaces The process of gradient descent is iterative, and include 3 steps: evaluate the quality of the current parameters (forward-pass step), get the error value, use the error-value in order to update the parameters/getting new set of parameters (back-propagation step). This process is repeated N times, which is decided by either not observing any update in the error, or we ran out of computational resources.\relax }}{59}{figure.caption.24}
\contentsline {figure}{\numberline {3.4}{\ignorespaces An example of using RNN in order to infer a sentence. The token to be generated here is a character. At each time step, the model is given the part of the sentence that has been generated so far, and asked to give the probability distribution over the next character. This distribution is then given to the selected sampling distribution, which sample the next character, and so on.\relax }}{62}{figure.caption.26}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Illustration of temperature sampling. When the temperature $\tau $ is very low, it becomes greedy sampling. With the increase of temperature, we can see more higher possibility of sampling the lower-probability tokens. When the temperature is too high, it becomes uniform sampling.\relax }}{64}{figure.caption.27}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Different conditioning method for RNN\relax }}{67}{figure.caption.33}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Using BLEU score of different sizes, we compare segments of variable length in the generated trace to the target trace.\relax }}{69}{figure.caption.35}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Left: architecture of the CNN letter classifier. Batch normalization is used after each convolution layer. The \textit {Dense 1} layer is the embedding that is used to condition our generator. Right: the autoencoder architecture we used. The first \textit {Dense 34} layer provides the latent space used to condition the generator.\relax }}{71}{figure.caption.37}
\contentsline {figure}{\numberline {3.9}{\ignorespaces The conditioned-GRU model used in this work. During the training mode \ref {subfig:dtl_training}, the input of the model is always the ground truth, and the predicted value is compared to the ground truth. During the generation mode \ref {subfig:dtl_generation}, the input to the model at each step is the model prediction from the previous step. The 'style info' and the 'task info' inputs are separated here for demonstration (they could be mixed).\relax }}{72}{figure.caption.38}
\contentsline {figure}{\numberline {3.10}{\ignorespaces a) In the the autoencoder latent space, there is no clear separation between letters; the encoding is based on the similarity of the images only. b) In the classifier embedding, there is a clear separation between the letters - with few exceptions -.\relax }}{75}{figure.caption.41}
\contentsline {figure}{\numberline {3.11}{\ignorespaces Examples of original letters. The blue \textit {x} mark is the starting point. These ones are generated using the letter + Writer bias. E and F are visually harder to recognize, since we do not model the pen pressure, otherwise, the rest of the letters are well recognizable.\relax }}{77}{figure.caption.44}
\contentsline {figure}{\numberline {3.12}{\ignorespaces Examples of generated letters. The blue \textit {x} mark is the starting point. These ones are generated using the letter + Writer bias. The general quality of this quite acceptable.\relax }}{78}{figure.caption.45}
\addvspace {10\p@ }
\contentsline {xchapter}{Framework}{79}{chapter.4}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Results of the manual annotation for the rotation of letter X drawings over the whole dataset. Almost half the writers drew X clockwise, the other half anti-clockwise. The undefined styles were unclear to determine.\relax }}{84}{figure.caption.51}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Projection for latent space for letter X using PCA. The colors show the ground truth of the X rotation: blue is counter clockwise, orange is clockwise, and the few red points are undefined.\relax }}{85}{figure.caption.52}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Examples for writing of letter X. Starting point is marked with the blue mark. Each raw is randomly sampled from each cluster in the bottleneck. The clusters shows that almost half the writers draw the letter clockwise (first row, first cluster), and the other half draw it anti-clockwise (second row, second cluster).\relax }}{86}{figure.caption.53}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Projection for latent space for letter C using t-SNE. The cluster surrounded by the red circle has a clear interpretation, where writers have a cursive style.\relax }}{87}{figure.caption.54}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Projection for latent space for letter A using PCA.\relax }}{88}{figure.caption.55}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Examples for writing of letter C from the selected cluster (first row) versus the rest of the letter drawings (second row). Starting point is marked with the blue mark. The drawings from the selected cluster show people with Edwardian style of handwriting.\relax }}{89}{figure.caption.56}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Examples for writing of letter A from the selected clusters. Starting point is marked with the blue mark. Each row is from one cluster. The first row show people who start drawing the letter from the top, going down, and then continue the drawing of the letter. The second row show people who start drawing from down directly.\relax }}{90}{figure.caption.57}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Projection for latent space for letter S using t-SNE. We manage to interpret the indicated cluster as the Edwardian style in drawing. The other two clusters (not indicated) did not show clear difference in the style, but this is an expected behavior from using the t-SNE algorithm, since it does not try to cluster styles as an objective.\relax }}{91}{figure.caption.58}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Examples for writing of letter S from the selected cluster (first row) versus the other two clusters (second row). Starting point is marked with the blue mark. The drawings from the selected cluster is always Edwardian style.\relax }}{92}{figure.caption.59}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Examples of generated letters. The blue mark is the starting point. The traces in green is the ground truth, and the red is the generated ones by our model.\relax }}{93}{figure.caption.60}
\contentsline {figure}{\numberline {4.11}{\ignorespaces Examples of generated letters. The blue mark is the starting point. The traces in green is the ground truth, and the red is the generated ones by our model.\relax }}{94}{figure.caption.61}
\addvspace {10\p@ }
\contentsline {xchapter}{Style Extraction and Transfer}{95}{chapter.5}
\contentsline {figure}{\numberline {5.1}{\ignorespaces Symmetric and Asymmetric transfer}}{99}{figure.caption.64}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Different proposed metrics to measure transfer learning \textbf {Mention the source of this image}\relax }}{99}{figure.caption.65}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Convolution Neural Networks filters shape}}{101}{figure.caption.67}
\contentsline {figure}{\numberline {5.4}{\ignorespaces QuickDraw! Confusion matrix for strokes for both baseline and transfer modes, on the different tasks.\relax }}{107}{figure.caption.75}
\contentsline {xpart}{Discussion and Closing Remarks}{111}{part.3}
\addvspace {10\p@ }
\contentsline {xchapter}{Discussion}{111}{chapter.6}
\addvspace {10\p@ }
\contentsline {xchapter}{Closing Remarks}{119}{chapter.7}
\addvspace {10\p@ }
\contentsline {xchapter}{List of publications}{121}{appendix.A}
