\chapter{Prespective and future work} \label{ch:discussion}
\minitoc% Creating an actual minitoc

This chapter will be a free discussion about what I had done, lessons learned, shortcomings of this work, difficulties in the PhD, and potential areas of development.

Science should always be about honesty, humility and respect, and not just flashy results and wide conclusions. Science can always make use of learning from setbacks -- something that is almost missing in the scientific literature --. I will do my best in this chapter to highlight the other side of good results.

Maybe one day someone -- maybe a PhD student -- will decide to follow on this work. It is important for me that they do not repeat the same mistakes. Instead, everything should be ready for them in order to make new mistakes. That is how we move forward. After all, we our objective in this PhD is transfer learning between different tasks. It is now time to transfer learning between two different colleagues.

\clearpage

\section{Challenges}
  \par In this section, I will discuss the challenges faced during the PhD -- from my personal perspective --, in the hope that I will learn from it for the future, and to illuminate the path for others as well.

  \subsection{Choice of how to tackle the topic?}
  \par The usage of deep learning in this topic has not been the first or clear option from the beginning. As I mentioned earlier, the objective of the project was to develop components for human-robot interaction domain. In a first glance, deep learning and human-robot interaction do not really mix well. The problem is simply the availability of data.

  \par There is a stretch of imagination in this thesis, that we assume -- and I believe rightly so -- that the data problem in HRI will be resolved in the future. Better and more reliable hardware is becoming available, and there is a general awareness now in the community about the need to do something concerning the data: a lot of data is being recorded by the research group, but there is no standardization or culture of open sourcing the data, even within the same team, leading to a big waste of efforts and time.

  \par At first, it seems that more data-efficient methods -- that depends on well designed priors from humans -- are the way to go in such project. However, the advances in deep learning application in areas like speech synthesis, image captioning, text and music generation, and the lucrative possibilities that data-driven approaches provide were hard to ignore. Besides, the current advances in machine learning indicate that computational approaches, even with simple algorithms, are outperforming methods that depends on human knowledge and prior\footnote{As nicely noted by Richard Sutton, one of the god fathers of reinforcement learning, in his article \textit{The bitter lesson}, \url{http://incompleteideas.net/IncIdeas/BitterLesson.html}}.

  \subsection{Determine the scope of interest in the state-of-the art}

    \par A major challenge during the PhD was to determine the relevant state of the art. For generative models, the usage of deep learning methods was not the clear choice of the beginning, and once chosen, it took considerable effort to determine the scope of the relevant literature.

    \par The same goes for the state of the art on styles. The word itself, and the range of study, is very wide -- as noted early in the introduction. By far, the work done in handwriting styles was the least relevant to our work; most of the work is done on offline handwriting (thus not dealing with the dynamics of writing itself) -- and this is most of the work done currently --\footnote{Most of the great advances that happened recently in neural networks -- especially in generative models -- is related to computer vision. Thus, it is more convenient to deal with with handwriting as images than as a dynamic process.}. The problem does not always manifest itself in a technical shape, sometimes -- and most annoying -- it is mostly that we do not know what we are looking for exactly, and even if we do know, we do not know the exact terminology other people are using to describe it.

    % \par The only way to tackle this issue was a brute force attack on the state of the art: read each and everything, decide the potential directions to focus on, build prototypes for these potential solutions, getting disappointed, tinker with the prototypes, and

    \par I am thankful for the great deal of openness that researchers in machine learning are embracing. The discussions through online forums, blogs, tutorials, online courses, and recent books, had definitely made this massive search space more tractable.

  \subsection{Lack of Benchmarks, evaluation metrics}
    Getting around these issues was quite a dilemma, for many reasons:

      \begin{itemize}
        \item I do not believe it is a healthy practice to pick up the benchmarks to use. This choice can be easily biased, and it could be argued that the benchmarks are chosen to be weak enough in order to show progress. In our case, it was mandatory to do so nevertheless, and we tried as much as possible to be fair in making these choices.
        \item In engineering, it is the right practice to have different teams for design and test of the product. If one team do both, the testing process tend to be biased (i.e., even with the best intention, the team is looking for confirmation of their design, not the problems in it). By analogy, I think this a pitfall of us developing the metrics, the benchmarks, and using them. What if we are mainly looking at the metrics that confirms our hypothesis? It is hard to rule out this possibility.
      \end{itemize}

    We tried our best to avoid this when selecting the metrics, and by using multiple metrics to evaluate our hypothesis. However, an independent investigation in this issue is favorable.

  \subsection{Deep learning: theory, hardware and software frameworks}
    \par Several excellent frameworks -- like \textit{Keras}~\citep{chollet2015keras}, \textit{PyTorch}~\citep{paszke2017automatic} and \textit{TensorFlow}~\citep{tensorflow2015-whitepaper} -- do exist at the moment in order to provide friendly APIs for deep learning, with many online tutorials. This gives the impression that you can just jump in the topic, train a neural network, and now you can harness the power of deep learning. However, in my experience, the quick gains of this approach will be lost soon in the face of the first problem. Even for someone experienced with traditional machine learning, deep learning poses an extra challenge: it is usually computationally expensive. More thinking is needed about what to do and what not to do in this case.

    \par It is important to understand the fundamental of statistical learning theory~\citep{hastie01statisticallearning}, machine learning and deep learning before engaging in an endeavor that uses deep learning. This particular strategy was a key factor in any progress done during my PhD. There are plenty of excellent online courses, free books and many resources, that provide a gradual and methodological approach, which a learner can use in order to achieve this.

    \par Another thing to highlight here is the hardware needs. Having access a GPU is a necessity in order to learn, experiment and develop using deep learning. Recently, interesting cloud-based solutions -- like \textit{Colab}\footnote{\url{https://colab.research.google.com/}}, which is free -- provide quick access to hardware suitable for deep learning. Some other solutions that I used -- like \textit{Amazon AWS}, which is not free -- do exist, with the advantage of being easily configured and scalable, and with a support from the research institution, could provide a good replacement for buying and maintaining expensive hardware in-house. The bottom line is: it is important to keep in mind the hardware available, otherwise, the whole process will be hindered.

    \par Another aspect to consider is the framework to use. \textit{Keras} for example provide interesting high-level APIs, while \textit{PyTorch} and \textit{TensorFlow} provide low-level APIs. \textit{PyTorch} focuses more on being close to the Python language way of thinking, while \textit{TensorFlow} provide a wide variety of interesting functions, including deployment capabilities\footnote{This gap between \textit{PyTorch} and \textit{TensorFlow} is closing, with every new version of both.}. It is lucrative to go for \textit{Keras}, but once a low-level development is needed, I find that it adds an unpleasant overhead, requiring a mastery level of the underlying framework. Besides, in my opinion, starting by working on high-level directly encourages bad practices -- since everything is done in background, it is easy to bypass important details in the way deep learning works, thus, developing poor debugging and problems diagnostic skills --. Discovering \textit{PyTorch} was, by far, the unspoken hero in this PhD, and one of the best engineering decisions I have made. My point from discussion is illuminate the different trade-offs between the different platform. No one is better than the other. It is important to understand the task in hand, and choose the suitable tool for it.

\section{Limitations of the current work}
  In this section, I discuss what I consider shortcomings for some of the methods used in this work.

  \subsection{Style extraction and exploration using PCA and tSNE methods}
    In this work, when exploring the latent space of our model, I used either PCA or tSNE projection methods (to project the latent space from the high dimensional space into a smaller one)\footnote{Or I used the latent space in a classification task to identify if particular information exists in the latent space.}, and tried to use the assumptions behind both methods to extract meaningful information from the latent space.

    While this is an acceptable approach, it really stretches these methods to a breaking point, plus, it may hinder further investigation:

    \begin{description}
      \item[PCA] It assumes orthogonality and linearity in the space to be projected. There is no reason however to assume that these assumptions hold for different styles.
      In the non-linearity aspect, the latent space does not have the clear objective of transferring non-linear style relationship into linear ones (simply, because no such objective can be formulated directly, since the problem of styles is ill-defined), unlike what can be noticed for the last layers of neural network classifiers (where an embedded objective of the network is project the data from their non-linear manifold into a linear one). Finding orthogonality in the style space is an interesting aspect to explore, but this is a strong assumption, and there is no reason to believe that it holds for all aspects of styles.
      \item[tSNE] It provides a way to deal with non-linearity, thus allowing another further exploring the latent space, but it is hard to repeat the results (the method is stochastic) and the projection does not necessarily yield information about the styles. Changing the \textit{perplexity} parameter leads to different results as well (I didn't explore the relation of that parameter to find a more suitable style manifold, and I am not sure if it is worth the effort).
    \end{description}

    But what is a good projection criteria in this case? should we let the organization of styles emerge on its own, by constraining the latent space and add regularization to the loss function (i.e., during an end-to-end training of the network)? should a second optimization step be performed on the latent space, in order to disentangle it? I discuss some of these ideas briefly in section~\ref{sec:future_direction}.

  \subsection{Leak in the style module}
    \par In a basic autoencoder (no condition on the bottleneck), one can assume that encoder part will learn the identity of the task + the style in the same time. The idea of conditioning is to provide the task description (aka: task identity) as an input to the decoder (the condition), thus, relieving the encoder from learning it, and focus only on learning the styles, thus enhancing the style transfer capability.

    Ideally, we expect that the output of the encoder has little to none information about the task identity. However, careful testing shows that this is not the case. There is a considerable leak of information about the task identity into the encoder.

    % \textbf{PUT THE RESULTS OF OUR TESTING HERE. TRY IF POSSIBLE TO COMPARE WITH A NORMAL AUTOENCODER.}

    I do not have an explanation at the moment for the reason behind this phenomena. My intuition\footnote{I did not have the time to perform rigorous testing for this idea unfortunately.} is that on aspect of the problem lies in the task description. The assumption that a harsh one-hot encoding of the task is sufficient to describe the task correctly is flawed in my opinion.

    An analogy for this can been drawn from clustering (hard clustering VS fuzzy clustering). Hard clustering, similar to one-hot encoding, does provide us with which this task is, but nothing about how this task relates to other tasks (i.e., proximity/similarity to other task), which is what fuzzy clustering do.

    The influential work done by Geoffrey Hinton in~\citep{hinton2015distilling} -- performed on the MNIST dataset~\citep{lecun-mnisthandwrittendigit-2010} -- is a contributing factor in this intuition. I will not dive into details about this article here, since it is outside the scope of this work, I will just mention two interesting results from this study:

    \begin{itemize}
        \item In a classification task, the traditional description of the labels is one-hot encoding. However, using a soft/fuzzy description of the labels reveals much better results (makes sense, since it is more rich in information).
        \item If you train a classifier on the soft labels of digits 7 and 8 only, the classifier will perform almost 90\% accuracy on the other labels \footnote{I personally find this particular result fascinating.}!. It means that a better task description may increase the data efficiency of the model.
    \end{itemize}
    A similar concept should definitely be explored in the context of this work.

\section{Future directions}\label{sec:future_direction}
  \par During my thesis, with each step, with each question answered, the door was unlocked to many new questions. This is the beautiful part about science. The not-so beautiful part is that time is limited, and choices have to be made, I can not pursue them all. I try to document what I believe is the possible directions to go from here in this section.
  % \textbf{Separate the technical parts (very tactical/low-level) from the research parts (strategic/high-level/long-range)}

  \subsection{Disentanglement of latent space to uncover styles}
    \par So far, we did not try to impose any constraints or structure on the latent space. We used post-processing methods (PCA, tSNE or classification) in order to shade some light on the content of the latent space. Adding structure to the latent space is very interesting, it may allow the emergence of these styles on their own. Some work has been done in that direction in case of images~\citep{higgins2017beta}. An example of adding structure can be simply by forming the latent space as a number of gaussian distributions~\citep{kingma2013auto}, enforcing discretization in the latent space~\citep{jang2016categorical,maddison2016concrete,van2017neural}. Another interesting dimension to explore is to have a hierarchical latent space structure~\citep{hsu2018hierarchical}, which can add more interpretability to the latent space, enabling a better comprehension of the extracted styles. In all these examples, the model is optimized end-to-end.

    \par Finding a good structure and constraints is probably a challenging task, but the potential rewards are huge, for two reasons:
    \begin{itemize}
      \item It can be an efficient way to discover and understand styles. After all, an important aspect of my PhD is to use machine learning as a tool to perform science, as discussed in the introduction.
      \item In a generative framework, this structure can be seen as control knobs for the generator. A good disentanglement will give us meaningful control knobs. Once we have them, we can start use them to synthesize new data with the characteristics that we want.
    \end{itemize}

    \par Another possible direction to look at a better post-processing methodology. For example, given some criteria, we can optimized the learned latent space in order to structure the latent space. I implicitly did this by using PCA and tSNE, but further development on this direction can be done.

  % \subsection{Separation of task into content and style}
  % \par So for in my PhD, I did not tackle the problem of defining and extracting the content from a task. The content in my thesis were known beforehand (the letter or shape categories), thus, allowing me to focus on the problem of styles. But at one point, this issue must be tackled. Some

  \subsection{Data efficiency}
    \par By data efficiency I mean that amount of data needed in order to achieve the desired performance. This depends on many things, including the data itself, the complexity of the distribution, the machine learning algorithm used...,etc. This point is important to consider when data is expensive, like in robotics for example. In my thesis, due to the availability of the data in this task, the problem of data efficiency did not surface and was not of concern. It can considered that transfer learning is one way to address data efficiency (by requiring less data samples in order to learn the target task). However, I strongly recommend having this point in mind, and having an idea about what are the actual limitations on data availability, which bring me to the next point...

  \subsection{Perceptual evaluation and system specification}
    \par In this thesis, we showed the validity of transfer learning in case of styles in an objective manner, using many performance metrics, which we believe that they matter and are relevant. The important point here is: how much difference in each performance metric do we actually care about? If I say that, concerning the EOS metric, that one system has a Krippendorff correlation of 0.9 and another one has 0.95, the question here is simply: should we be concerned about this 0.05 difference? and how much difference should concern us? And if we do care about this 0.05 difference, then is it worth the effort spent in order to get it (time wise, model complexity...,etc)? It is very important to go from the numerical universe to the physical universe, and get a sense of what those numbers actually mean, and determine what do care for.

    \par This leads to another important point, which is to determine what do we actually want (i.e., develop the necessary specifications and criteria for our final objective). In case of human-robot interaction for example -- I will use arbitrary numbers here --, we can consider that the robot can try a particular action with the human three times only in order to get it right (maybe the human will get bored after). Thus we need to consider algorithms that can update their decisions efficiently. Another question could be about the acceptable level of performance that should be achieved in order to have a successful interaction. This will give good guidance for the development of new algorithms.

    \par Bottom line is, it is important to know what we want first, and what is the limitation that we have, otherwise, the development can be easily misguided. It is important that when we do machine learning, we do machine learning that actually matters~\citep{wagstaff2012machine}.

  % \begin{itemize}
  %   \item Embedding (robust generalization/maybe style extraction)
  %   % \item Multi-stage optimization (for style extraction) (research)
  %   % \item Disentanglement of styles / latent space distributions / loss OR constraint (for knowledge extraction): interesting for extracting new styles AND building behavior generators. (research)
  %   \item RL and planning to reduce the complexity/have control over the generation and evaluation process (not sure)
  %   \item Memories of the neural network for better task decomposition (useful for transfer learning) -- maybe report my experiment with the external memory here. (research)
  %   \item Statistical testing for neural network performance and for the inference quality + the effect of the random seed. (engineering/deployment)
  %   \item Better task description (research)
  %   % \item Automatic separation of task and style (research)
  %   % \item Data efficiency and Model Complexity (engineering/deployment)
  %   \item Define data augmentation protocol: while many techniques exists for images, it is not so clear in case of sequences. (research)
  %   \item \textbf{Treating dynamical system as an image, with coloring gradients representing the dynamics of the system}: this will give us massive power in using the SOTA of generative models developed for static images (based on GANs), while addressing all the characteristics of the dynamical system in the same time.
  %
  %   This is similar to treating sound waves as images (the ML course I did with Washington University), or spectrum images (like what i did with Marielle).
  %
  %   \item Dealing with style extraction as an embedding problem: \textbf{NOTE: i am not sure if we didn't do that already. Probably in the shaping of the latent space or the loss function. Or maybe it is already okay now. }
  %
  %   \item Modeling the error distribution of the model during the generation phase: in order to enhance the quality of the generation, and reduce the distance between the generation and the prediction (i.e., the training and the usage phases) \footnote{nothing, quick test}.
  %
  %   \item World models paper: having a model of the world, and use it to train a proper generator using RL.
  %
  %   \item Re-visiting MDN (even the in discrete case): a useful tool, even for finite-discrete distribution. Mixed with good embedding, and giving 'uncertainty' level, provides important information. This also has a very good potential for optimization.
  %
  %   \item Using RL for generation instead of log-likelihood models, and free the metric and the study from the constraints of differentiability.
  %   \item The hyper-parameter tuning: Do we need it? And how to do it efficiently? (yes we do need it. the question is how to do it in an efficient manner).
  %
  %   \item Evaluate the quality of the content/task generated, not just the style: usually done via subjective testing or using another model -- which i strictly refuse --. Subjective testing is too slow.
  %
  %   \item What do these numbers actually mean in real life? how much should we care?~\citep{wagstaff2012machine} (machine learning that matters paper): we used multiple metrics in order to compare/assess our models. But the main issue is how much this really matter? at how much $X\%$ difference in the performance that humans start to perceive the difference for example? This is very important in order to reach ML that actually matters in real life.
  %
  %   \item The experimental protocol and scientific practices:
  %
  %   \item Use of CNN instead of RNN for the style extraction (embedding) part: more flexible and faster, many off-the-shelf components to choose from (in order to build the network)
  %
  %   \item Changing the name of the BLEU score metric, since it created a confusion with the way we use it, relative to the way to was created for.
  % \end{itemize}
