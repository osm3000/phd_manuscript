\chapter{Hyper-parameter tuning}
\par Machine learning in its core is a search in the hypothesis space of a particular algorithm, in order to find the suitable parameters/hyper-parameters that best fit the data, while adhering the rules of statistical learning theory \citep{hastie01statisticallearning}. For parameters tuning in neural networks (i.e., learning the weights of the networks), back-propagation \citep{rumelhart1988learning} is one of the well established algorithms most commonly used\footnote{Several other algorithms do exist, like evolutionary algorithms \citep{eiben2003introduction} -- which recently are achieving remarkable results --.}.

\par Yet, when it comes to hyper-parameters, it is not that obvious\footnote{By not obvious, I am politely meaning it was an utter suffering!}. There is two main strategies for finding hyper-parameters\footnote{As wonderfully explained by 'Andrew Ng' in his \textit{Deep Learning Specialization} in \textit{Coursera} platform, \url{https://www.coursera.org/specializations/deep-learning}}:
\begin{itemize}
  \item Babysitting the model: for people with small computational
  \item Using a search strategy: one of the dominant search strategies in this case is a random search in the hyper-parameter space.
\end{itemize}
