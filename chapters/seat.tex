\chapter{Style Extraction and Transfer} \label{ch:seat}
\minitoc% Creating an actual minitoc

% \textbf{Missing points}
% \begin{itemize}
%   \item Add a diagram explaining transfer Learning
%   \item Add a diagram explaining which part of the model we are transferring, and which one we are retraining (maybe a series of diagrams explaining visually all the steps)
%   \item Add examples of the generated letters/shapes
% \end{itemize}

\par We finally arrive to the core objective of our thesis: how to leverage information of style from one (or more) task/s, in order to bootstrap the learning of a new task?

\begin{mdframed}[backgroundcolor=blue!20]
  \begin{center}
    Questions addressed in this chapter
  \end{center}

  \begin{itemize}
    \item What is transfer learning? and what are the different approaches to perform it? We will explain the different paradigms and metrics used to characterize transfer learning.
    \item How do we approach the problem of style transfer, for both handwriting and sketch drawing?
    \item The experiments performed, the results, and our conclusions.
  \end{itemize}
\end{mdframed}

\clearpage

% \section{Background}
  % \begin{itemize}
  %     \item Explain different paradigms and metrics used to characterize transfer learning.
  %     \item Why extracting style?: on our side, as ML practitioners, the ultimate objective of using ML is to help us extract new knowledge
  %     \item The questions of research here: extracting styles from bottleneck, transfer between writers, transfer between tasks
  % \end{itemize}

  % \par Before we dive into the technical details, it is important to motivate why do we need to perform this in the first place. A common scenario is that we collect data over time for a particular task. This process can be quite expensive (e.g., collecting data from the robots is a slow and expensive process). Then, a new task of interest emerges. This task has common aspects with the old one. The question here is if we can leverage this common aspects, in order to bootstrap the learning the new task.

  % For example, in case of handwriting, when trying to write new letters, and need to have the skill of using the pen, and performing micro-actions, like curves and straight lines. In the case of new letters, we need to recombine those micro-actions with new actions, in order to draw the new letters. Another example, in case of humans, is grasping objects. If you learn how to hold a plastic bottle, then you try to hold a glass cup, there are a lot of atomic actions in common, but the forces you apply are different.

  % \par Examples of transfer learning:
  % \begin{itemize}
  %     \item If you know how to hold a glass cup, with little adjustments, you can learn how to hold a plastic bottle.
  %     \item If you know probability and algebra, you can use this knowledge in order to bootstrap/accelerate your progress in mastering machine learning.
  % \end{itemize}

  % \par There are plenty of cases where transfer learning is/could be useful, like:
  % \begin{itemize}
  %   \item In robotics~\citep{konidaris2012robot,Konidaris:2012:TRL:2188385.2343689}, collecting data can be quite expensive process, due to hardware limitations from one side, and human limitatio as well (in case of human-robot interaction scenarios). In addition, with techniques like reinforcement learning~\citep{sutton2018reinforcement}, where the agent learns by trial and error, the process can be prohibitively slow, with safety concerns sometimes.
  %   \item In underwater acoustics~\citep{malfante2018automatic}, an essential task is collecting and cleaning the data about the different fish sounds. This is a tediously manual job, and any change (type of fish, time of the day or place in the ocean) degrades the quality of prediction a lot. Transfer learning can be very useful in this case, to reduce the effort needed to collect, clean and annotate the data.
  % \end{itemize}


\section{Transfer learning}\label{sec:transfer_learning}
  \par An important research direction in machine learning nowadays is transfer learning. If humans and machines are able to learn how to perform a task, one of the thing that separates humans from machines is the ability to leverage this knowledge in order to acquire new skills and perform new tasks, without the need for additional trials and errors from tabula rasa. This however, is not a straightforward thing for machine learning to do. The algorithms are fitted to data responding directly to the task required (i.e., has the same input feature space and same distribution). Thus, a change in the task can lead to degradation in the algorithm performance~\citep{shimodaira2000improving,pan2009survey,weiss2016survey,dtl2018survey}.

  % \par In the following subsections, I first introduced notations and clear description for the objective of transfer learning, and the metrics used to evaluate the quality of transfer. I then discuss different types of transfer learning, with examples on each type.

  % \subsection{Notation and problem definition}
  \par Let's first introduce some notations that will help in formulating the problem:
  \begin{itemize}
      \item We first introduce the concept of \textit{Domain}. A domain defines a feature space (e.g., images of animals), and the probability distribution of this space (i.e., the distribution of pixels in the images of animals). We can consider the domain as the available \textit{knowledge} to us. Thus, a domain $\mathcal{D}$ is defined as $\mathcal{D} = \{\mathcal{X}, P(X)\}$, where:\\
      $\mathcal{X}$ is the feature space, $X$ is the data samples available to us from the feature samples, $X = \{x_1,x_2,\cdots,x_n\} \in \mathcal{X}$, where $n$ is the size of the learning sample. $P(X)$ is the marginal distribution probability of this data sample.

      \item For a given domain (aka, the knowledge available to us), a task is something we would like to achieve using the knowledge we have. A task $\mathcal{T}$ is defined as $\mathcal{T} = \{\mathcal{Y}, f(.)\}$, where:\\
      $\mathcal{Y}$ is the task objectives, $f(.)$ is the mapping function (mapping the domain knowledge to the task objectives). It can also be rewritten as a conditional probability over the domain knowledge, $\mathcal{T} = \{\mathcal{Y}, P(Y|X)\}$.

      \item Based on this notation, we define two more concepts: \textit{Source} and \textit{Target}. A source defines a domain and a task/s that are available to us already (where we have plenty of domain knowledge, and examples on the task/s). A target defines a domain and a task/s as well, where we usually do not have enough domain knowledge and/or examples on the task/s.
  \end{itemize}

  \par Now that we clarified some basic terminology, we can move on to define transfer learning: given source domain data $D_S$, source task $\mathcal{T}_S$,  target domain $D_T$ and target task $\mathcal{T}_T$, we wish to improve the performance of the target task $f_T(.)$ by using $D_S$ and $\mathcal{T}_S$.

  \par Given this definition, we can categorize different types of problems that transfer learning covers:

  \begin{itemize}
      \item The source and target domains are different, $D_S \neq D_T$, which means that the feature space is different, $\mathcal{X}_S \neq \mathcal{X}_T$, and/or the probability distribution of the feature space are not the same, $P(X_S) \neq P(X_T)$. If $\mathcal{X}_S \neq \mathcal{X}_T$, the transfer learning problem is \textit{Heterogeneous}. Otherwise, it is \textit{Homogeneous}.

      \item The source and target tasks are different, $\mathcal{T}_S \neq \mathcal{T}_T$, which means that the objectives are different, $\mathcal{Y_S} \neq \mathcal{Y_T}$, and/or the mapping function (from the feature space to the objectives) are different, $P(Y_S|X_S) \neq P(Y_T|X_T)$.
  \end{itemize}

  \par Many approaches in order to achieve transfer learning has been proposed in the literature. We will discuss the different approaches, and give examples from the literature on each one. We follow the categorization of transfer learning approaches done in~\citep{dtl2018survey}\footnote{Other categorization exists, like the one used in~\citep{weiss2016survey}. When it comes to deep transfer learning, we believe the categorization in~\citep{dtl2018survey} to be the most relevant.}, by first identifying four main categories of transfer learning:
  % Either talk about the other types of transfer learning (that I mention here), or completely ignore them all.
  \begin{itemize}
    \item \textit{Instances-based}: in this case, we utilize examples from the source domain into the training of the new target domain, by defining weights on them.
    \item \textit{Mapping-based}: the objective in this case is to project the instances from the two domains into a new manifold, that increases the similarity between the two domains.
    \item \textit{Network-based}: the more common type of deep transfer learning. It is based on the idea that the layers of the deep neural network extracts basic and general information, that shared a lot with other domains. In this case, the network or some of its layers are re-used on the target task.
    \item \textit{Adversarial-based}: similar objective to \textit{Mapping-based}, by using generative adversarial networks~\citep{goodfellow2014generative} in order to find a manifold that are fit for both source and the target domains.
  \end{itemize}

  \par In the context of \textit{deep transfer leaning} -- the main domain in our work --, \textit{Network-based} and \textit{Adversarial-based} transfer are the relevant categories in this case. \textit{Adversarial-based} transfer is quite recent, and there is not much to talk about at the moment, so we will focus \textit{Network-based} transfer, as it is the most common type of deep transfer learning.

  \par When it comes to evaluating the success of the transfer, there is no one way to evaluate transfer learning in general. This depends a lot on the objectives of transfer learning, and the criteria of success. In the case of machine learning, the improvement in end quality of the model is the primary performance aspect being measured and reported -- like the classification accuracy~\citep{chattopadhyay2012multisource,long2013transfer,pan2010cross,glorot2011domain}, the reduction in the average error~\citep{pan2010domain}...,etc --. The transfer is considered successful if it achieves better performance than the baseline method (a model trained from scratch on the data available for the target task only).

  \par We expect that, with the introduction of transfer learning via deep learning, that the \textit{time to train} the model could be another aspect to consider -- similar to what is being used in reinforcement learning~\citep{taylor2007cross} --, although -- to the best of our knowledge -- we do not find studies mentioning this at the moment.

  % \subsection{Network-based transfer learning}
  \par The idea of using deep learning~\citep{lecun2015deep} in order to achieve transfer learning has gain popularity during the last years, following the achievements in having better computational resources~\citep{raina2009large}, and the availability of large benchmark datasets - most notably: ImageNet~\citep{imagenet_cvpr09} for object detection, MS-COCO~\citep{2014arXiv1405.0312L} for image captioning~\ldots.

  \par The first notable success of deep learning happened in the area of computer vision, with the AlexNet architecture~\citep{krizhevsky2012imagenet}. It was found out that such a deep network manages to extract generic features about the images: it learns simple, hierarchical filters, that are generic enough to be applicable for different datasets (see figure~\ref{fig:AlexNet_filters}). The filters can be also seen as a representation of knowledge learned on the given tasks, with the first layers learning primitive filters (like edge detection for example), and the subsequent layers learning more complex filters. This observation led to another surge in the usage of pre-trained AlexNet -- and later newer architectures, like VGG16~\citep{simonyan2014very}, Inception~\citep{szegedy2015going}...,etc -- as feature extractors for new, unseen datasets (i.e, transfer learning using these deep learning architectures).

  \begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.4]{images/sota/filt1.jpeg}
    % \caption{Visualization of the first convolution layer of a trained AlexNet. The weights are very nice and smooth, indicating nicely converged network. Note also the basic simple shape of filters. It easy to imagine that those same filters will be useful in other computer vision or image related tasks.}
    \caption[Convolution Neural Networks filters shape]{Visualization of the first convolution layer of a trained AlexNet. Note the basic shape of filters that resemble to Gabor filters widely used in image processing for decades~\citep{fogel1989gabor,jain1991unsupervised}. It is possible to see that those same filters will be useful in other computer vision or image-related tasks.}
    \label{fig:AlexNet_filters}
  \end{figure}

  % \par It is interesting to note that those filters can be also seen as a representation for the skills we want to extract. In our case, we will need to combine both deep convolution network with a recurrent neural network (like in~\citep{s16010115,pinheiro2014recurrent,huang2016deep}).
  \par Other examples on network-based transfer learning are:
  \begin{itemize}
    \item \textit{Sentiment Classification}:~\citep{glorot2011domain} discusses a deep learning approach for transfer learning for sentiment classification, by using stacked de-noising auto-encoders~\citep{vincent2008extracting} to correct the marginal distribution between the source and the target domain, by learning latent variables/features common between the two data sources in two steps: they first train an auto-encoder on the unlabeled data from the source and the target. This will produce latent variables that will make $P(X_S)$ closer to $P(X_T)$. They then use those latent features to train a classifier on the labeled source data.

    Experiments are done on 12 different sources and target domain pairs. The data are reviews for different products (4 different products). The performance metric used in this case is the classification error rate when using a classifier trained on the source task only, minus the classification error rate of a classifier trained on the target task only. A SVM classifier trained on the source domain is used as a baseline, and a comparison with other transfer methods~\citep{blitzer2006domain,li2008multi,pan2010cross}. All these methods performs better than the baseline, and~\citep{glorot2011domain} peforms better than all of them.

    \item \textit{Underwater Acoustics}:~\citep{malfante2018use} compares the use of deep transfer learning to manual features~\citep{malfante2016automatic,malfante2018machine}, in order to recognize the sounds of fish underwater. Interestingly, the deep learning models are trained on ImageNet~\citep{imagenet_cvpr09} dataset (which is completely unrelated to acoustics), then used in order to extract features from the spectrogram of the underwater recordings. The assumption here is the the filters learned with deep learning are basic and generic enough, to be used in other domains, thus, deep learning can provide a natural correction for $P(X_S)$, to make it close to $P(X_T)$. Without any fine-tuning, the deep learning achieves quite a high performance, although less than the state of the art, suggesting that a further investment in this direction is worth the time.

    \item \textit{Speech to speech translation}:~\citep{xu2014cross} studies the idea of transferring the speech knowledge learned from one language to another using deep neural networks. In particular, they study the transfer between Mandarin and English (both ways). Their hypothesis is that when modeling the speech in a language, there is a part in the model that is language independent. Their work was to find this part, and use it to bootstrap the learning of a new language (in case of neural networks, this can be rephrased as trying to find the good part in the neural network that is language independent).

    They compared their proposed transfer approach to a baseline model (a deep learning model trained on the target task only), and compare the different the usage of different layers of a deep neural network trained on the source task (in order to determine the best part, or the language independent part). They report multiple performance metrics: segmental SNR (SSNR), log-spectral distortion (LSD), and perceptual evaluation of speech quality (PESQ). They find that, given insufficient data on the target task (which is one of the motivations to perform transfer learning), this transfer scheme works better than the baseline.

    \item \textit{Image classification}:~\citep{oquab2014learning} investigated the usage of a convolution neural network (CNN) trained on ImageNet~\citep{imagenet_cvpr09} (where the object of interest is centered in the image), to extract low-level and mid-level features, that can transfer well to more complex dataset, PASCAL VOC~\citep{everingham2010pascal} (where there are several objects of interest in the image), and is smaller than ImageNet. They use the \textit{average precision} as their performance metric, and they show that the transfer is better than training a model from scratch on this target dataset.

    \item \textit{Styles of speech synthesis}:~\citep{wang2018style} discusses the problem of transferring the styles between different speakers, in the task of speech synthesis. Traditionally, the outcome of speech synthesis systems is the same style (average reading). One of the challenges is to capture the richness of style of different speakers on some training data, and transfer this style to new text (in particular, when the readers change their voice between different characters in the text). In their work, they propose an embedding approach called \textit{Global Style Tokens} (GST), in order to extract the styles from the different speakers during the training phase. They then show that they can use these tokens as extra information to the speech synthesis system, to bias/affect the style of the outcome.
    They compare this to a baseline model, called \textit{TACOTRON}~\citep{wang2017tacotron}, without these GST addition. They use \textit{Mean Opinion Score} (MOS) performance metric -- a subjective metric to assess the quality of experience experienced --. They conclude that transfer learning using GST outperforms the baseline model.

  \end{itemize}

  % \subsection{Adversarial-based transfer learning}
  % \par Encouraged by the recent advances in generative adversarial networks (GANs)~\citep{goodfellow2014generative}, a new line work is trying to utilize this tool to achieve transfer learning, on the assumption that a good representation (suitable for transfer) should be general enough that it does not discriminate between the source and the target domains, yet discriminative enough to learn the required tasks on either domains.
  %
  % \subsection{Negative transfer}
  % \par A concern that arises in transfer learning is what is called \textit{negative transfer}, where the knowledge learned from one task leads to no improvement on the target task, or reduction in the quality of learning for that task. This can happen for multiple reasons~\citep{rosenstein2005transfer}:
  % \begin{itemize}
  %     \item The source task is not sufficiently related to the target task, thus, in the best case scenario, no useful knowledge can be transferred.
  %     \item The transfer method is not able to exploit the similarities between the source and the target task
  % \end{itemize}
  % In~\citep{rosenstein2005transfer}, the authors studied this problem, and provided examples for negative transfer.
  % Within the framework of \textit{reinforcement learning}, some methods can estimate task similarity~\citep{taylor2008transferring, torrey2010transfer}, though these methods do not provide any theoretical guarantees about their effectiveness. This, however, is an open area of investigation in the framework of \textit{supervised learning} \textbf{CAN'T FIND RESOURCES HERE}. The main thing here for us to be careful during the choice/design of the source and the target experiments, to ensure that a possible positive transfer learning can happen.

\section{Putting it all together}
  \par In the previous section, we explored the concept of transfer learning, with focus on network-based transfer, with multiple examples from the literature on this type of transfer. In this section, we will explain the work done during our thesis on transfer learning. The main challenge we had is how to capture and transfer the styles between different tasks. Two questions should be of concern:
  \begin{itemize}
    \item Which framework/methodology of transfer we should use?
    \item How to assess the quality of transfer?
  \end{itemize}

  \par Our contributions in this part is that we study these questions on both \textit{IRONOFF} and \textit{QuickDraw!}. We proposed an experimental protocol in order to perform a rigorous evaluation for the usage of transfer learning -- partially on the basis of the work done in~\citep{8686063} --. We use a slightly different experimental protocol between the two datasets however, depending on the computational power available. We will discuss the reasoning behind this, and will argue that both setups are good enough to address our questions concerning the transfer of styles. The work done in this chapter is in the publication phase.


  \paragraph{What are we transferring?} A typical approach to perform transfer learning in deep learning models is to:
    \begin{enumerate}
      \item Train the model on the source task.
      \item Freeze some parts of that model.
      \item When going to the target task, use the frozen parts in a new model, and train the rest of that new model on the target task.
    \end{enumerate}

    The part that we freeze/transfer in our case is illustrated in figure~\ref{fig:transfer_part_illustration}.
  \begin{sidewaysfigure}
    \includegraphics[scale=0.95]{./images/sota/transfer_part.jpeg}
    \caption{An illustration for model we use. The part identified by the gray area -- the style extraction module -- is what we transfer (i.e., what we freeze from the source task, and use in the target task). Since we give the model the task content/identity, the gray part is expected to focus more on the style extraction. During the exposure to the source task, all the different components of the model are being trained. During the exposure to the target task, the gray area is the trained one on the source task, with frozen parameters. The other parts will be trained.}
    \label{fig:transfer_part_illustration}
  \end{sidewaysfigure}

  \paragraph{How to evaluate the quality of transfer?} Concerning the performance metrics, we decided to use multiple metrics to determine the effect of transfer learning, and compare it to the baseline (no transfer). While the usage of a single metric offers more convenience for decision making (i.e., which model to choose), our goal is have a better understanding for the transfer of style. To achieve this, we would like to shade light on the outcome from different angles, by using multiple metrics. We first evaluate the quality of the model in prediction by looking at the log-likelihood on the test data\footnote{Based on experience, the cross-entropy matters when it comes to generation. A difference of around 0.1 in cross-entropy between two models gives different generation quality.}. We then evaluate the quality of generation using our previously proposed metrics (the BLEU score and EOS analysis). We compare the performance of the generated strokes count, and include it in the comparison.

  \par One of the things we changed from the previous work is that, when analyzing \textit{end of sequence}, instead of using Pearson Coefficient, we use Krippendorff-Alpha~\citep{krippendorff2011computing}. The reason for this is that it offers a smoother transition between what is good and bad (thus, it is a good way to deal with ordinal values). In Pearson coefficient, if there is a mismatch between the generation and ground truth, it will not matter how big is the mistake (the distance between the generated and the ground truth). With Krippendorff-alpha, we can take into account the distance between the generated and the target symbols, thus providing a softer and more realistic estimation for the correlation.

  \par Another aspect we consider in our analysis is the number of \textit{strokes} the model is generating. In the previous work, we did not model the strokes, in order to simplify the system and the analysis. Now that we are confident about our approach, we added the strokes to our model. Strokes are more complex to model, because the stroke signals are more sparse. A good stroke generation is an indication that the model can perform hard discrete decisions in order to generate the whole shape. We use Krippendorff coefficients as well to report and analyze the strokes. We also consider the confusion matrices for the generated strokes versus the ground truth ones, in order to shade more light on the behavior of the model.

  \par One important choice to make is to decide the amount of data to be used in the target task. Normally, the whole point from transfer learning is to cope with the insufficient data available for the target task. However, since we do not have a particular scenario to solve (we are mainly interested in that 'what if this happens' question), there is no clear criteria to decide the amount of data. If we select very few points, we may be biased towards giving transfer learning an advantage, while also sacrificing having generally bad results (even if transfer learning proved to be better than baseline on our performance metrics). It is important that the overall behavior of the system is acceptable (generating shapes with acceptable quality). To get around this design problem, we decided to use the entire data available to the target task, thus, testing the 'worst case scenario' for transfer learning (where the baseline has plenty of data, and it is used to train the whole model, compared to training only half the model in case of transfer learning). This will be the case for both \textit{IRONOFF} and \textit{QuickDraw!} datasets.

  \par To summarize, our contributions in this chapter are:
  \begin{itemize}
    \item We show how to use the conditioned-autoencoder framework in order to perform style transfer.
    \item We propose two protocols -- different in power -- in order to perform the experiments of transfer learning.
    \item We intensively evaluate the usage of transfer learning relative to the baselines, across a wide range of metrics.
    \item We perform the tests on two different datasets, for extra confirmation.
  \end{itemize}

  \subsection{IRONOFF}
    % \par In case of handwriting letters, we define the task by the higher category it belongs to. So we have 3 tasks: uppercase, lowercase and digits\footnote{I do not see a problem of working on the lower categories (A, a, B, b, ...., 7, 8, 9). However, it would have consumed a lot of computational time test on all possible combinations of source/target tasks.}.
    \par In case of handwriting letters, we distinguish between three tasks: uppercase, lowercase and digits\footnote{I do not see a problem of working on the lower categories (A, a, B, b, ...., 7, 8, 9) -- each letter/digit separately --. However, it would have consumed a lot of computational time test on all possible combinations of source/target tasks.}.
    \par We explore the idea of transfer learning on all possible combinations of tasks:
    \begin{itemize}
      \item From uppercase and lowercase to digits,
      \item from uppercase and digits to lowercase,
      \item and from lowercase and digits to uppercase.
    \end{itemize}

    \subsubsection{Experimental setup}
      \par Figure~\ref{fig:ironoff_protocol} details the protocol we used for this experiment. For each source/target task combination, we first perform hyper-parameters search for the network for both the source and the target tasks, and then train these models. Using these models:
      \begin{itemize}
        \item We use the source model to extract the encoder module (aka, the style extraction module). We then add it to a new model, freeze it (it will not be part of training), and train the new model on the target task. We also search for the best hyper-parameters for this new model. We call this model the 'transfer model'. We retrain this transfer model 5 times with different random weights each time, and report the stats on the cross-entropy of the test data for those repetitions.
        \item We retrain the target model 5 time as well.
        \item Then, we use the best performing transfer and targets models for generating the target tasks, and report the stats of the different generation metrics.
      \end{itemize}
      \par We use 10\% of the data for testing, another 10\% for validation, and the rest is our training set.

    \begin{sidewaysfigure}[htbp]
      \includegraphics[scale=0.3]{images/sota/ironoff_results/IRONOFF_transfer_learning_flow.png}
      \caption[\textit{IRONOFF} experimental protocol]{Flow chart explaining the experiment protocol used in \textit{IRONOFF} dataset.}
      \label{fig:ironoff_protocol}
    \end{sidewaysfigure}

    \subsubsection{Results}

      \paragraph{Loglikelihood of prediction} In the prediction mode, the model is tested in a similar manner to the way it was trained: it is given the input from the ground truth, representing the current time step, and asked to predict the next time step. This tells us about the quality of the training procedure from one side, and shade light on the confidence of the model in predicting the next time step.

      The result of 5 times repetitions, for all the different combinations of source/target tasks, are mentioned in figure~\ref{fig:ironoff_crossentropy}. We can see that the transfer learning always gives a significant advantage over baseline models.
      \begin{sidewaysfigure}
        \centering
          \includegraphics[width=0.48\textwidth]{images/sota/ironoff_results/Uppercase.png}\quad
          \includegraphics[width=0.48\textwidth]{images/sota/ironoff_results/Lowercase.png}\quad
          \includegraphics[width=0.48\textwidth]{images/sota/ironoff_results/Digits.png}
        \caption[\textit{IRONOFF} - log cross-entropy of prediction results for different tasks]{\textit{IRONOFF}: log cross-entropy of prediction of test dataset for different combinations of source/target tasks, with 5 repetitions. We can see that, in all possible source/target task combinations, the transfer learning gives better advantage than just learning from scratch on the target task. The stars indicate the statistical significance level (1 for $<0.05$, 2 for $<0.01$ and 3 for $<0.001$).}
        \label{fig:ironoff_crossentropy}
      \end{sidewaysfigure}

      % \begin{table}[!htbp]
      %   \centering
      %   \begin{tabular}{|l|c|c|}
      %   \hline
      %   Task of comparison & P-Value \\ \hline
      %   Uppercase &  0.00076 \\ \hline
      %   Lowercase & 0.0017 \\ \hline
      %   Digits & 0.00026 \\ \hline
      %   \end{tabular}
      %   \caption{\textit{IRONOFF}: the statistical significance analysis (p-value) -- using Wilcoxon test -- between transfer learning and baselines on the crossentropy of prediction, for all the tasks, shows that transfer learning gives a clear advantage over the baselines.}
      %   \label{table:ironoff_crossentropy_pvalue}
      % \end{table}

    \paragraph{BLEU score}
      Now we go to the generation part, our main concern in our thesis. As discussed earlier, we use BLEU score to assess the quality of matching segments between the generated and the ground truth letters, in a gradual manner (i.e., we increase the size of the segments to match gradually). Table~\ref{table:bleu_score_ironoff_transfer} summarizes the numbers. We see that transfer learning always performs better than the baseline.
    \paragraph{End-of-Sequence analysis}
    One aspect to measure the quality of the generation, that we identified previously, is to analyze how the model predicts the end of the sequence generation. Table~\ref{table:ironoff_eos_transfer} shows the results of the Krippendorff coefficients for the different modes. In general, the different modes are performing quite well. It can be seen that transfer learning is actually performing better than the baseline models, adding another indication to the benefit of using transfer learning.

    \paragraph{Strokes analysis}
    As mentioned earlier in this section, we started to consider the the strokes in this part of our work. Table~\ref{table:ironoff_strokes_transfer} shows the strokes results of the Krippendorff coefficients for the different modes. With the exception of the uppercase letters, transfer learning performs better than baseline models. For the uppercase letters, this could be due to the extra complexity of these letters (see figure~\ref{fig:ironoff_strokes}, where it can be found that uppercase letters are usually the ones with higher number of strokes). A fine tuning for the style extraction module is the next logical step to perform here, to make it adapt to complexity of the this task. We also consider the confusion matrix for the generated strokes in comparison with the ground truth, figure~\ref{fig:ironoff_strokes_cnf}. We can see that it is consistent with the Krippendorff results, where transfer learning outperforms the baselines. In the uppercase letters however, it is noted that the transfer learning performs better on single stroke, while the baseline performs better on the 2 and 3 strokes.

      \begin{table}[!htbp]
        \centering
        \begin{tabular}{l c c c|c c c}
          \hline
          \multicolumn{1}{l}{Aspect/Feature} & \multicolumn{3}{c}{ Speed } & \multicolumn{3}{c}{ Freeman }   \\ \hline
          Model / B-score      & B-1  & B-2  & B-3           & B-1  & B-2   & B-3              \\ \hline
          Uppercase-baseline &   66.1 & 46.3 & 27.2 & 62.8 & 49.4 & 37.1 \\%\hline
          \textbf{Uppercase-transfer} &   \textbf{68.3} & \textbf{47.8} & \textbf{28.3} & \textbf{65.47} & \textbf{51.8} & \textbf{39.0} \\\hline

          Lowercase-baseline &   73.1 & 69.7 & 55.9 & 54.8 & 37.2 & 40.9 \\%\hline
          \textbf{Lowercase-transfer} &   \textbf{75.5} & \textbf{71.2} & \textbf{58.0} & \textbf{56.0} & \textbf{39.4} & \textbf{41.9} \\\hline

          Digits-baseline &    68.7 & 65.2 & 49.1 & 49.6 & 29.3 & 34.6  \\%\hline
          \textbf{Digits-transfer} &    \textbf{71.5} & \textbf{70.7} & \textbf{51.2} & \textbf{55.9} & \textbf{31.4} & \textbf{41.7} \\\hline

        \end{tabular}
        \caption{\textit{IRONOFF}: BLEU score results on the generated letters, for the baseline models (trained on the target task only), and the transfer models (the encoder -- style extractor -- is trained on the source task, while the decoder is trained on the target task). The results show the advantage of using transfer learning.}
        \label{table:bleu_score_ironoff_transfer}
      \end{table}

      \begin{table}[!htbp]
        \centering
        \begin{tabular}{l c c} \hline
        Task/Model & Baseline & Transfer \\ \hline
        Uppercase & 0.95 &  \textbf{0.97}\\ %\hline
        Lowercase & 0.96 & \textbf{1.0} \\ %\hline
        Digits & 0.92 & \textbf{0.99} \\ \hline
        \end{tabular}
        \caption{\textit{IRONOFF}: Krippendorff correlation coefficients for the End-Of-Sequence (EoS) distributions between the transfer and baseline, for all tasks.}
        \label{table:ironoff_eos_transfer}
      \end{table}

      \begin{table}[!htbp]
        \centering
        \begin{tabular}{l c c} \hline
        Task/Model & Baseline & Transfer \\ \hline
        Uppercase & \textbf{0.38} & 0.25 \\ %\hline
        Lowercase & 0.56 & \textbf{0.65} \\ %\hline
        Digits & 0.4  & \textbf{0.71} \\ \hline
        \end{tabular}
        \caption{\textit{IRONOFF}: Krippendorff correlation coefficients for the strokes distributions between the transfer and baseline, for all tasks. Except for the uppercase case, transfer learning seems to perform well in the lowercase and the digits tasks.}
        \label{table:ironoff_strokes_transfer}
      \end{table}

      % Strokes conf matrix
      \begin{figure}[!htbp]
        \centering
        \begin{subfigure}[tb]{0.45\textwidth}
            \includegraphics[width=\textwidth]{images/sota/ironoff_results/True_upper_strokes_heatmap.png}
        \end{subfigure}
        ~
        \begin{subfigure}[tb]{0.45\textwidth}
            \includegraphics[width=\textwidth]{images/sota/ironoff_results/False_upper_strokes_heatmap.png}
        \end{subfigure}

        ~
        \begin{subfigure}[tb]{0.45\textwidth}
            \includegraphics[width=\textwidth]{images/sota/ironoff_results/True_lower_strokes_heatmap.png}
        \end{subfigure}
        ~
        \begin{subfigure}[tb]{0.45\textwidth}
            \includegraphics[width=\textwidth]{images/sota/ironoff_results/False_lower_strokes_heatmap.png}
        \end{subfigure}

        ~
        \begin{subfigure}[tb]{0.45\textwidth}
            \includegraphics[width=\textwidth]{images/sota/ironoff_results/True_digits_strokes_heatmap.png}
        \end{subfigure}
        ~
        \begin{subfigure}[tb]{0.45\textwidth}
            \includegraphics[width=\textwidth]{images/sota/ironoff_results/False_digits_strokes_heatmap.png}
        \end{subfigure}

        \caption{\textit{IRONOFF!}: Confusion matrix for strokes for both baseline (left) and transfer (right) models, on the uppercase, lowercase letters and digits (in order). A small difference may appear in the total number between the baseline and the transfer confusion matrix is a result of us removing the column when it has a low/insignificant value (for aesthetic reason only, and to make the message focused).}
        \label{fig:ironoff_strokes_cnf}

      \end{figure}

  \subsection{QuickDraw!}
    \par In case of sketch drawings, we define the task by the class it belongs to. So we have 5 tasks: circle, triangle, square, hexagon and octagon. We explore the idea of transfer learning on all possible combinations of tasks: in each combination, one task is removed (the target task), and the other tasks are considered as source tasks.

    \subsubsection{Experimental setup}
    \par Due to the increase of the number of tasks -- compared to \textit{IRONOFF} --, and using the same experiment setup proved to be too much compared to the computational resources available to us. We identify that one of the expensive bottlenecks in our work is the number of times we need to perform hyper-parameter search. In order to get around this problem, we are using a simpler (and less powerful protocol), see figure~\ref{fig:quickdraw_protocol}. We perform the hyper-parameters search once in the beginning of the experiment, on the all the tasks combined, to find good hyper-parameters that is suitable to the domain of 'sketch drawing'. From there, we fix the hyper-parameters for all the different steps in the experiment. The analysis steps are the same as in \textit{IRONOFF}.

    Another thing we noticed is that retraining the model 5 times only (like in \textit{IRONOFF}) leads to unstable conclusions, thus make it hard to quantify the difference between the baseline and the transfer modes. We increased the number of models to 30 in this case, in order to have a more consistent trend.

    \begin{sidewaysfigure}[htbp]
      \includegraphics[scale=0.3]{images/sota/quickdraw_results/QuickDraw_transfer_learning_flow.png}
      \caption{Flow chart explaining the experiment protocol used in \textit{QuickDraw!} dataset.}
      \label{fig:quickdraw_protocol}
    \end{sidewaysfigure}

    \subsubsection{Results}
      \paragraph{Loglikelihood of prediction}
        The results can be seen in figure~\ref{fig:quickdraw_crossentropy}. For all the combinations, there is a clear advantage for using transfer learning over the baseline model. The difference in all cases is statistically significant.
        \begin{sidewaysfigure}
          \centering
            \includegraphics[width=0.31\textwidth]{images/sota/quickdraw_results/cir_ce.png}\quad
            \includegraphics[width=0.31\textwidth]{images/sota/quickdraw_results/tri_ce.png}\quad
            \includegraphics[width=0.31\textwidth]{images/sota/quickdraw_results/sq_ce.png}\quad
            \includegraphics[width=0.31\textwidth]{images/sota/quickdraw_results/hex_ce.png}\quad
            \includegraphics[width=0.31\textwidth]{images/sota/quickdraw_results/oct_ce.png}\quad
          \caption{\textit{QuickDraw!}: cross-entropy of prediction of test dataset for different combinations of source/target tasks, with 30 repetitions. We can see that, in all possible source/target task combinations, the transfer learning gives better advantage than just learning from scratch on the target task. The stars indicate the statistical significance level (1 for $<0.05$, 2 for $<0.01$ and 3 for $<0.001$).}
          \label{fig:quickdraw_crossentropy}
        \end{sidewaysfigure}

        % \begin{table}[!htbp]
        %   \centering
        %   \begin{tabular}{|l|c|c|}
        %   \hline
        %   Task of comparison & P-Value \\ \hline
        %   Circle & 0.0003  \\ \hline
        %   Triangle & 5.15e-10  \\ \hline
        %   Square & 9.9e-12  \\ \hline
        %   Hexagon & 0.01  \\ \hline
        %   Octagon & 1.4e-05  \\ \hline
        %   \end{tabular}
        %   \caption{\textit{QuickDraw!}: the statistical significance analysis (p-value) -- using Wilcoxon test -- between transfer learning and baselines on the crossentropy of prediction, for all the tasks, shows that transfer learning gives a clear advantage over the baselines.}
        %   \label{table:quickdraw_crossentropy_pvalue}
        % \end{table}

      \paragraph{BLEU score}
        The BLEU score results are summarized in table~\ref{table:bleu_score_quickdraw_transfer}. Transfer learning outperforms the baseline models in this case.

        \begin{table}[!htbp]
          \centering
          \begin{tabular}{l c c c|c c c}
            \hline
            \multicolumn{1}{l}{Aspect/Feature} & \multicolumn{3}{c}{ Speed } & \multicolumn{3}{c}{ Freeman }   \\ \hline
            Model / B-score      & B-1  & B-2  & B-3           & B-1  & B-2   & B-3              \\ \hline
            Circle-baseline &    59.0 & 54.5 & 49.6 & 60.0 & 54.7 & 48.9 \\%\hline
            \textbf{Circle-transfer} &    \textbf{70.4} & \textbf{65.5} & \textbf{60.3} & \textbf{65.0} & \textbf{58.1} & \textbf{50.6} \\\hline

            Triangle-baseline &  47.3 & 40.0 & 32.6 & 33.2 & 28.2 & 24.0  \\%\hline
            \textbf{Triangle-transfer} &  \textbf{61.3} & \textbf{52.4} & \textbf{44.1} & \textbf{50.6} & \textbf{44.8} & \textbf{39.8}  \\\hline

            Square-baseline &    46.8 & 40.1 & 32.7 & 44.0 & 39.1 & 34.9  \\%\hline
            \textbf{Square-transfer} &    \textbf{57.9} & \textbf{50.8} & \textbf{42.9} & \textbf{53.0} & \textbf{47.4} & \textbf{42.3} \\\hline

            Hexagon-baseline &   58.1 & 50.4 & 41.4 & 45.4 & 40.3 & 35.9   \\%\hline
            \textbf{Hexagon-transfer} &   \textbf{62.0} & \textbf{54.0} & \textbf{44.8} & \textbf{47.6} & \textbf{42.3} & \textbf{37.8}  \\\hline

            Octagon-baseline &   55.2 & 47.1 & 38.3 & 43.7 & 38.7 & 34.6   \\%\hline
            \textbf{Octagon-transfer} &   \textbf{57.3} & \textbf{49.3} & \textbf{40.5} & \textbf{46.1} & \textbf{41.1} & \textbf{36.7}  \\\hline

          \end{tabular}
          \caption{\textit{QuickDraw!}: BLEU score results on the generated letters, for the baseline models (trained on the target task only), and the transfer models (the encoder -- style extractor -- is trained on the source task, while the decoder is trained on the target task). The results show an advantage in using transfer learning.}
          \label{table:bleu_score_quickdraw_transfer}
        \end{table}

      \paragraph{End of Sequence analysis}
        Table~\ref{table:quickdraw_eos_transfer} summarizes the results on end-of-sequence analysis in case of \textit{QuickDraw!}. The benefits of transfer learning are clear.
        \begin{table}[!htbp]
          \centering
          \begin{tabular}{l c c} \hline

          Task/Model & Baseline & Transfer \\ \hline
          Circle &  0.6 & \textbf{0.84} \\ %\hline
          Triangle & -0.05 & \textbf{0.61} \\ %\hline
          Square &  0.04 &  \textbf{0.35} \\ %\hline
          Hexagon &  0.07 &  \textbf{0.2} \\ %\hline
          Octagon &  0.07 &  \textbf{0.16} \\ \hline

          \end{tabular}
          \caption{\textit{QuickDraw!}: Krippendorff correlation coefficients for the end-of-sequence distributions between the generated letters and the ground truth letters.}
          \label{table:quickdraw_eos_transfer}
        \end{table}

      \paragraph{Strokes analysis}
        Table~\ref{table:quickdraw_strokes_transfer} summarize the results on strokes analysis in case of \textit{QuickDraw!}. The benefits of transfer learning are clear. We also report the confusion matrix for the strokes, figure~\ref{fig:quickdraw_strokes_cnf}. It is noted that the performance of the system in general decreases, as the diversity in the number of strokes increases. But still, transfer learning provides the better results.

        \begin{table}[!htbp]
          \centering
          \begin{tabular}{l c c} \hline
          Task/Model & Baseline & Transfer\\ \hline
          Circle &  -0.04 &  \textbf{0.1} \\ %\hline
          Triangle & -0.04 & \textbf{0.42} \\ %\hline
          Square &  0.03 &  \textbf{0.25} \\ %\hline
          Hexagon & -0.08 & \textbf{0.23} \\ %\hline
          Octagon & 0.06 & \textbf{0.18} \\ \hline

          \end{tabular}
          \caption{\textit{QuickDraw!}: Krippendorff correlation coefficients for the strokes distributions between the generated letters and the ground truth letters. Transfer learning achieves better results than the baseline on all the different tasks.}
          \label{table:quickdraw_strokes_transfer}
        \end{table}

    % Strokes conf matrix
    \begin{figure}[!htbp]
      \centering
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_circle_target_strokes_heatmap.png}
      \end{subfigure}
      ~
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_circle_transfer_strokes_heatmap.png}
      \end{subfigure}

      ~
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_triangle_target_strokes_heatmap.png}
      \end{subfigure}
      ~
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_triangle_transfer_strokes_heatmap.png}
      \end{subfigure}

      ~
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_square_target_strokes_heatmap.png}
      \end{subfigure}
      ~
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_square_transfer_strokes_heatmap.png}
      \end{subfigure}

    \end{figure}

    % Strokes conf matrix - continue
    \begin{figure}[!htbp]\ContinuedFloat

      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_hexagon_target_strokes_heatmap.png}
      \end{subfigure}
      ~
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_hexagon_transfer_strokes_heatmap.png}
      \end{subfigure}

      ~
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_octagon_target_strokes_heatmap.png}
      \end{subfigure}
      ~
      \begin{subfigure}[tb]{0.45\textwidth}
          \includegraphics[width=\textwidth]{images/sota/quickdraw_results/quickdraw_octagon_transfer_strokes_heatmap.png}
      \end{subfigure}

      \caption{\textit{QuickDraw!} Confusion matrix for strokes for both baseline (left) and transfer (right) models, on the different tasks. A small difference may appear in the total number between the baseline and the transfer confusion matrix is a result of us removing the column when it has a low/insignificant value (for aesthetic reason only, and to make the message focused).}
      \label{fig:quickdraw_strokes_cnf}
    \end{figure}

\subsection{A word of caution about confusion matrix}
\par In the previous two experiments, we presented the confusion matrix for the strokes. However, it is important to understand what these numbers actually mean. In a normal classification problem, the confusion matrix gives important information about the precision and recall of the classification. When comparing two algorithms -- for example --, the usage of the values of confusion matrix is an acceptable approach to consider. However, this is not the case when dealing with the generative aspects of machine learning. Bear in mind the asymmetry between the training of the model (using the log-likelihood of prediction) and the usage of the model (by sampling from the model), thus, a one-to-one comparison between what is generated and the ground truth is not possible or meaningful. A logical consequence for this is that we can not use such a method to compare two models together (i.e., compare transfer learning and the baseline). What we would like to achieve with generative model is to \textit{capture} the distribution of the ground truth, thus, for a meaningful comparison, we want to compare the distribution of the generation relative to the distribution of the ground truth. That is a core challenge in our thesis: to provide tools to capture the distribution, to facilitate the comparison and evaluation of different models/methods.

\par One way to look at the confusion matrix is the general trends in the matrix, like:
\begin{itemize}
  \item The deviation around the diagonal: as the shapes gets more complicated, and as the number strokes increases, we expect that the deviation will increase.
  \item Looking for systematic errors (e.g., the number of strokes for a square is always one instead of being three or four): this could help diagnosing problems with the models design, data processing...,etc.
\end{itemize}

\section{Are we actually capturing styles?} \label{ch:seat_sec:rf}
\par In the previous section, we showed that all the metrics point to the benefits of transfer learning over using the target data only. We argued that this approach captures the styles, and that we are transferring the styles to a new task. But, are we really capturing the styles this way? In section~\ref{ch:framework_sec:styleperletter}, we tried to motivate this point, by some exploring the bottleneck of the styles. We showed that looks consistent with what we know beforehand (in case of letter 'X' for example), or that it uncovers things we either did not notice beforehand (like in letters 'C' and 'S'), or uncover new things that we did not anticipate (in case of letter 'A').

\par Testing for something that is ill-defined and not always known beforehand, like styles, is quite challenging. To add to the challenge, the methods we used to explore the bottleneck space (PCA and tSNE) are not designed to uncover a 'styles' manifold, so sometimes, basic styles went missing (for example, with this exploration methods, we could not uncover the clockwise/counterclockwise drawing of letter 'O' for example, which we know a priori that it exists!).

\par In this section, we want to shade more light on this problem, from a different and quantifiable angle. We will choose one aspect using \textit{QuickDraw!} dataset this time, we manually annotated circles and octagons into clockwise/counterclockwise categories. Then, we build a classifier on top of the styles bottleneck, to measure the quality of the capturing of this style aspect. If we can not see this style visually with PCA and tSNE, then we should be able to measure its impact.

\par We annotated 716 drawings (see table~\ref{table:cw_ccw_annotation}). We trained a classifier (\textit{Random forests} classifier) on this data directly, randomly separating training and test data, and repeated this 5 times. We did not configure the hyper-parameters of that classifier (we want a general figure, not interested in an exact one). We obtained $92.9 \pm 6.0 \%$ on the weighted F1 score. This suggests that this style extraction method does indeed capture this style aspect.

\begin{table}[!htbp]
  \centering
  \begin{tabular}{l c c}
  \hline
  Task/Direction & Clockwise (CW) & Counter-clockwise (CCW)\\ \hline
  Circle &  339 & 57 \\ %\hline
  Octagon & 115 & 205 \\ \hline
  \end{tabular}
  \caption{Results of manual annotation for CW-CCW on 716 drawings (octagon/circles) in \textit{QuickDraw!} dataset. Sometimes the drawing is not clear, so we did not include. The selected examples are the clear ones only. It can be seen that the data is not balanced.\footnote{We normally would not expect that for a circle. We would expect a balanced distribution. However, take into account that we are people are using the mouse for drawing here, so the expected 'pen' dynamics do not always exist.}}
  \label{table:cw_ccw_annotation}
\end{table}


\section{Summary and take-away message}
\par In this chapter, we presented our hypotheses for style transfer learning. We presented our proposed approach, and the proposed experimental protocol in order to investigate these hypotheses. To have better support for our conclusions, we carried out the experiments on two different datasets, \textit{IRONOFF} and \textit{QuickDraw!}, both of them presenting different challenges and behaviors: \textit{IRONOFF} has more clear semantics, made by a pen, while in \textit{QuickDraw!} the contributors exerted more freedom on their work, and most of the work is believed to be done by the mouse or maybe a tablet (using a finger or a pen). Also, we chose to perform the study on the worst case scenario, where there is an abundance of data available to the target task. To better understand and compare transfer learning versus the baseline models (models trained only on the target task), we considered multiple performance metrics: the log-likelihood of prediction, the BLEU score, end-of-sequence and strokes analyses for the generated shapes.

\par The results overwhelmingly point to the benefit of transfer learning compare to the baselines on the different proposed metrics, thus providing a strong evidence to our hypotheses.
